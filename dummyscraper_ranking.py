# -*- coding: utf-8 -*-
"""dummyscraper-ranking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yXcIvyNFWdMQwAqVwvqdcAwyWhZhpn_s
"""

!pip install --upgrade numpy==1.26.4
!pip install --upgrade gensim==4.3.3
!pip install --upgrade scipy==1.13.1

import requests
from bs4 import BeautifulSoup

def scrape_google_news(query, num_results=20):
    query = query.replace(' ', '+')
    url = f"https://www.google.com/search?q={query}&tbm=nws&num={num_results}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36"
    }

    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, "html.parser")

    news_results = []
    for el in soup.select("div.SoaBEf"):
        try:
            link = el.find("a")["href"]
            title = el.select_one("div.MBeuO").get_text()
            snippet = el.select_one(".GI74Re").get_text()
            date = el.select_one(".LfVVr").get_text()
            source = el.select_one(".NUnG9d span").get_text()
            news_results.append({
                "title": title,
                "link": link,
                "snippet": snippet,
                "date": date,
                "source": source
            })
        except Exception:
            continue

    return news_results

# Example usage:
if __name__ == "__main__":
    topic = "artificial intelligence"
    articles = scrape_google_news(topic, num_results=20)
    for i, article in enumerate(articles, 1):
        print(f"{i}. {article['title']}")
        print(f"   Source: {article['source']}")
        print(f"   Date: {article['date']}")
        print(f"   Link: {article['link']}")
        print(f"   Snippet: {article['snippet']}\n")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np


texts = [a["title"] + " " + a["snippet"] for a in articles]

# Compute TF-IDF matrix
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(texts)

# Compute cosine similarity matrix
cosine_sim_matrix = cosine_similarity(tfidf_matrix)

# Calculate average cosine similarity for each article (excluding self)
avg_sim_scores = []
for i in range(len(articles)):
    sum_sim = np.sum(cosine_sim_matrix[i]) - cosine_sim_matrix[i][i]  # exclude self
    avg_sim = sum_sim / (len(articles) - 1) if len(articles) > 1 else 0
    avg_sim_scores.append(avg_sim)

# Attach the average similarity score to each article
for i, article in enumerate(articles):
    article['avg_similarity'] = avg_sim_scores[i]

# Rank the articles by their average similarity scores in descending order
ranked_articles = sorted(articles, key=lambda x: x['avg_similarity'], reverse=True)

# Output the ranking
for idx, art in enumerate(ranked_articles, 1):
    print(f"{idx}. {art['title']}")
    print(f"   Average similarity: {art['avg_similarity']:.4f}")
    print(f"   Source: {art['source']}")
    print(f"   Date: {art['date']}")
    print(f"   Link: {art['link']}\n")

